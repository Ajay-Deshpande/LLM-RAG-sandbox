{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "971b8eb2-54ca-4b16-b046-079d526b406e",
      "metadata": {
        "id": "971b8eb2-54ca-4b16-b046-079d526b406e"
      },
      "outputs": [],
      "source": [
        "! pip install langchain_community tiktoken langchain-google-genai langchainhub chromadb langchain cohere langchain-cohere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a66c5fd1-8da9-416f-b854-1b70d207d606",
      "metadata": {
        "id": "a66c5fd1-8da9-416f-b854-1b70d207d606"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "os.environ['LANGSMITH_PROJECT'] = \"langsmith_project_Retrieval_Flow\"\n",
        "os.environ['LANGCHAIN_API_KEY'] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aeb22812-822d-4320-be3c-ab0c52356914",
      "metadata": {
        "id": "aeb22812-822d-4320-be3c-ab0c52356914"
      },
      "outputs": [],
      "source": [
        "os.environ[\"GOOGLE_API_KEY\"] = \"\"\n",
        "os.environ['COHERE_API_KEY'] = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0ae5a7f-7c07-4825-b53b-4ac632b8eac6",
      "metadata": {
        "id": "a0ae5a7f-7c07-4825-b53b-4ac632b8eac6"
      },
      "source": [
        "## Re-ranking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f201eec1-6ed0-4236-9594-286894574779",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f201eec1-6ed0-4236-9594-286894574779",
        "outputId": "faca5f80-b50c-4441-bd84-9aef956387b8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ],
      "source": [
        "#### INDEXING ####\n",
        "\n",
        "# Load blog\n",
        "import bs4\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_cohere import CohereEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_cohere import ChatCohere\n",
        "from langchain.load import dumps, loads\n",
        "from operator import itemgetter\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_community.llms import Cohere\n",
        "from langchain.retrievers import  ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import CohereRerank\n",
        "\n",
        "\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "blog_docs = loader.load()\n",
        "\n",
        "# Split\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=50)\n",
        "\n",
        "# Make splits\n",
        "splits = text_splitter.split_documents(blog_docs)\n",
        "\n",
        "# Index\n",
        "vectorstore = Chroma.from_documents(documents=splits,\n",
        "                                    embedding=CohereEmbeddings(model = \"embed-english-v2.0\"))\n",
        "\n",
        "\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b733a230-d217-4ee0-8482-ab4380e7be4f",
      "metadata": {
        "id": "b733a230-d217-4ee0-8482-ab4380e7be4f"
      },
      "outputs": [],
      "source": [
        "# RAG-Fusion\n",
        "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
        "Generate multiple search queries related to: {question} \\n\n",
        "Output (4 queries):\"\"\"\n",
        "prompt_rag_fusion = ChatPromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "6e37bc5d-93fc-4b34-8a4d-208394e89709",
      "metadata": {
        "id": "6e37bc5d-93fc-4b34-8a4d-208394e89709"
      },
      "outputs": [],
      "source": [
        "generate_queries = (\n",
        "    prompt_rag_fusion\n",
        "    | ChatCohere(model = \"command-r\", temperature=0)\n",
        "    | StrOutputParser()\n",
        "    | (lambda x: x.split(\"\\n\"))\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "728fedac-7f1d-49aa-9d02-47243c0f2bb4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "728fedac-7f1d-49aa-9d02-47243c0f2bb4",
        "outputId": "ef4f7011-f055-4622-ed7e-60aaced8d5bd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def reciprocal_rank_fusion(results: list[list], k=60):\n",
        "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents\n",
        "        and an optional parameter k used in the RRF formula \"\"\"\n",
        "\n",
        "    # Initialize a dictionary to hold fused scores for each unique document\n",
        "    fused_scores = {}\n",
        "\n",
        "    # Iterate through each list of ranked documents\n",
        "    for docs in results:\n",
        "        # Iterate through each document in the list, with its rank (position in the list)\n",
        "        for rank, doc in enumerate(docs):\n",
        "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
        "            doc_str = dumps(doc)\n",
        "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
        "            if doc_str not in fused_scores:\n",
        "                fused_scores[doc_str] = 0\n",
        "            # Retrieve the current score of the document, if any\n",
        "            previous_score = fused_scores[doc_str]\n",
        "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
        "            fused_scores[doc_str] += 1 / (rank + k)\n",
        "\n",
        "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
        "    reranked_results = [\n",
        "        (loads(doc), score)\n",
        "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    ]\n",
        "\n",
        "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
        "    return reranked_results\n",
        "\n",
        "question = \"What is task decomposition for LLM agents?\"\n",
        "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
        "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "ee43bb94-613d-46a3-b688-02c2d2aadfba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "ee43bb94-613d-46a3-b688-02c2d2aadfba",
        "outputId": "d5903c50-d410-4181-8e42-939a51bda6a6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'For agents powered by LLMs, task decomposition involves breaking down complex tasks into simpler, more manageable subgoals. This is a crucial planning step that enables the agent to handle complicated tasks more efficiently. By decomposing tasks, the agent can follow a step-by-step approach and improve its problem-solving capabilities. \\n\\nThere are several ways to achieve task decomposition:\\n1. Using simple prompts to request the LLM to provide steps or subgoals.\\n2. Providing task-specific instructions, such as outlining steps for writing a story.\\n3. Incorporating human inputs to guide the task decomposition process.\\n\\nThis technique is similar to how humans approach complex tasks and makes the LLM-based agent more effective in tackling intricate assignments.'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# RAG\n",
        "template = \"\"\"Answer the following question based on this context:\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "llm = ChatCohere(model = \"command-r\", temperature=0)\n",
        "\n",
        "final_rag_chain = (\n",
        "    {\"context\": retrieval_chain_rag_fusion,\n",
        "     \"question\": itemgetter(\"question\")}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "final_rag_chain.invoke({\"question\":question})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "b88e1ad8-57eb-40d4-8475-a50775a692b0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b88e1ad8-57eb-40d4-8475-a50775a692b0",
        "outputId": "b2bba454-79d7-4c12-a6bd-82be4b2129e7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-457c0b799379>:4: LangChainDeprecationWarning: The class `CohereRerank` was deprecated in LangChain 0.0.30 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-cohere package and should be used instead. To use it run `pip install -U :class:`~langchain-cohere` and import as `from :class:`~langchain_cohere import CohereRerank``.\n",
            "  compressor = CohereRerank()\n",
            "<ipython-input-14-457c0b799379>:9: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  compressed_docs = compression_retriever.get_relevant_documents(question)\n"
          ]
        }
      ],
      "source": [
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
        "\n",
        "# Re-rank\n",
        "compressor = CohereRerank()\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=retriever\n",
        ")\n",
        "\n",
        "compressed_docs = compression_retriever.get_relevant_documents(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5lzJOBn0YxIx",
      "metadata": {
        "id": "5lzJOBn0YxIx"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
