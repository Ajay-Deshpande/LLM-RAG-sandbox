{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install huggingface_hub\n%pip install mlflow>=2.11.0\n%pip install transformers peft accelerate bitsandbytes datasets -q -U","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T03:49:34.054160Z","iopub.execute_input":"2025-02-12T03:49:34.054486Z","iopub.status.idle":"2025-02-12T03:50:00.659397Z","shell.execute_reply.started":"2025-02-12T03:49:34.054453Z","shell.execute_reply":"2025-02-12T03:50:00.658290Z"}},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npandas-gbq 0.25.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m102.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.6/336.6 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from huggingface_hub import login\nlogin()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T04:12:55.889143Z","iopub.execute_input":"2025-02-12T04:12:55.889442Z","iopub.status.idle":"2025-02-12T04:12:55.915650Z","shell.execute_reply.started":"2025-02-12T04:12:55.889407Z","shell.execute_reply":"2025-02-12T04:12:55.914821Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d08c3ca021584cfc8d2bdecfe6898646"}},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import load_dataset\nfrom IPython.display import HTML, display\n\ndataset_name = \"b-mc2/sql-create-context\"\ndataset = load_dataset(dataset_name, split=\"train\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T03:50:00.660795Z","iopub.execute_input":"2025-02-12T03:50:00.661148Z","iopub.status.idle":"2025-02-12T03:50:01.779271Z","shell.execute_reply.started":"2025-02-12T03:50:00.661109Z","shell.execute_reply":"2025-02-12T03:50:01.778573Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T03:50:45.375873Z","iopub.execute_input":"2025-02-12T03:50:45.376169Z","iopub.status.idle":"2025-02-12T03:50:45.381428Z","shell.execute_reply.started":"2025-02-12T03:50:45.376147Z","shell.execute_reply":"2025-02-12T03:50:45.380534Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['answer', 'question', 'context'],\n    num_rows: 78577\n})"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"def display_table(dataset_or_sample):\n    # A helper fuction to display a Transformer dataset or single sample contains multi-line string nicely\n    pd.set_option(\"display.max_colwidth\", None)\n    pd.set_option(\"display.width\", None)\n    pd.set_option(\"display.max_rows\", None)\n\n    if isinstance(dataset_or_sample, dict):\n        df = pd.DataFrame(dataset_or_sample, index=[0])\n    else:\n        df = pd.DataFrame(dataset_or_sample)\n\n    html = df.to_html().replace(\"\\\\n\", \"<br>\")\n    styled_html = f\"\"\"<style> .dataframe th, .dataframe tbody td {{ text-align: left; padding-right: 30px; }} </style> {html}\"\"\"\n    display(HTML(styled_html))\n\n\ndisplay_table(dataset.select(range(3)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T04:08:36.592039Z","iopub.execute_input":"2025-02-12T04:08:36.592337Z","iopub.status.idle":"2025-02-12T04:08:36.603482Z","shell.execute_reply.started":"2025-02-12T04:08:36.592315Z","shell.execute_reply":"2025-02-12T04:08:36.602827Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style> .dataframe th, .dataframe tbody td { text-align: left; padding-right: 30px; } </style> <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>answer</th>\n      <th>question</th>\n      <th>context</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>SELECT COUNT(*) FROM head WHERE age &gt; 56</td>\n      <td>How many heads of the departments are older than 56 ?</td>\n      <td>CREATE TABLE head (age INTEGER)</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>SELECT name, born_state, age FROM head ORDER BY age</td>\n      <td>List the name, born state and age of the heads of departments ordered by age.</td>\n      <td>CREATE TABLE head (name VARCHAR, born_state VARCHAR, age VARCHAR)</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>SELECT creation, name, budget_in_billions FROM department</td>\n      <td>List the creation year, name and budget of each department.</td>\n      <td>CREATE TABLE department (creation VARCHAR, name VARCHAR, budget_in_billions VARCHAR)</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"split_dataset = dataset.train_test_split(test_size=0.2, seed=42)\ntrain_dataset = split_dataset[\"train\"]\ntest_dataset = split_dataset[\"test\"]\n\nprint(f\"Training dataset contains {len(train_dataset)} text-to-SQL pairs\")\nprint(f\"Test dataset contains {len(test_dataset)} text-to-SQL pairs\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T04:07:48.377143Z","iopub.execute_input":"2025-02-12T04:07:48.377501Z","iopub.status.idle":"2025-02-12T04:07:48.409770Z","shell.execute_reply.started":"2025-02-12T04:07:48.377470Z","shell.execute_reply":"2025-02-12T04:07:48.409072Z"}},"outputs":[{"name":"stdout","text":"Training dataset contains 62861 text-to-SQL pairs\nTest dataset contains 15716 text-to-SQL pairs\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Prompt Template\n\nPROMPT_TEMPLATE = \"\"\"You are a powerful text-to-SQL model. Given the SQL tables and natural language question, your job is to write SQL query that answers the question.\n\n### Table:\n{context}\n\n### Question:\n{question}\n\n### Response:\n{output}\"\"\"\n\n\ndef apply_prompt_template(row):\n    prompt = PROMPT_TEMPLATE.format(\n        question=row[\"question\"],\n        context=row[\"context\"],\n        output=row[\"answer\"],\n    )\n    return {\"prompt\": prompt}\n\n\ntrain_dataset = train_dataset.map(apply_prompt_template)\ndisplay_table(train_dataset.select(range(1)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T04:08:41.995125Z","iopub.execute_input":"2025-02-12T04:08:41.995415Z","iopub.status.idle":"2025-02-12T04:08:48.163369Z","shell.execute_reply.started":"2025-02-12T04:08:41.995392Z","shell.execute_reply":"2025-02-12T04:08:48.162709Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/62861 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47fb25bc3f574ce2800d9d987e8fcdb9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style> .dataframe th, .dataframe tbody td { text-align: left; padding-right: 30px; } </style> <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>answer</th>\n      <th>question</th>\n      <th>context</th>\n      <th>prompt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>SELECT perth FROM table_name_56 WHERE gold_coast = \"yes\" AND sydney = \"yes\" AND melbourne = \"yes\" AND adelaide = \"yes\"</td>\n      <td>Which Perth has Gold Coast yes, Sydney yes, Melbourne yes, and Adelaide yes?</td>\n      <td>CREATE TABLE table_name_56 (perth VARCHAR, adelaide VARCHAR, melbourne VARCHAR, gold_coast VARCHAR, sydney VARCHAR)</td>\n      <td>You are a powerful text-to-SQL model. Given the SQL tables and natural language question, your job is to write SQL query that answers the question.<br><br>### Table:<br>CREATE TABLE table_name_56 (perth VARCHAR, adelaide VARCHAR, melbourne VARCHAR, gold_coast VARCHAR, sydney VARCHAR)<br><br>### Question:<br>Which Perth has Gold Coast yes, Sydney yes, Melbourne yes, and Adelaide yes?<br><br>### Response:<br>SELECT perth FROM table_name_56 WHERE gold_coast = \"yes\" AND sydney = \"yes\" AND melbourne = \"yes\" AND adelaide = \"yes\"</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\nbase_model_id = \"mistralai/Mistral-7B-v0.1\"\n\n# You can use a different max length if your custom dataset has shorter/longer input sequences.\nMAX_LENGTH = 256\n\ntokenizer = AutoTokenizer.from_pretrained(\n    base_model_id,\n    model_max_length=MAX_LENGTH,\n    padding_side=\"left\",\n    add_eos_token=True,\n)\ntokenizer.pad_token = tokenizer.eos_token\n\n\ndef tokenize_and_pad_to_fixed_length(sample):\n    result = tokenizer(\n        sample[\"prompt\"],\n        truncation=True,\n        max_length=MAX_LENGTH,\n        padding=\"max_length\",\n    )\n    result[\"labels\"] = result[\"input_ids\"].copy()\n    return result\n\n\ntokenized_train_dataset = train_dataset.map(tokenize_and_pad_to_fixed_length)\n\nassert all(len(x[\"input_ids\"]) == MAX_LENGTH for x in tokenized_train_dataset)\n\ndisplay_table(tokenized_train_dataset.select(range(1)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T04:14:46.433105Z","iopub.execute_input":"2025-02-12T04:14:46.433400Z","iopub.status.idle":"2025-02-12T04:15:45.809439Z","shell.execute_reply.started":"2025-02-12T04:14:46.433376Z","shell.execute_reply":"2025-02-12T04:15:45.808542Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/996 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27227ac39ad14a789d5cb4c8386c4d06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ef4e19e895444a19b1f2ecc4000c475"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9467862e7c29444ba6ed5a15a867618c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bce2a594ca3945dc93800a14afa4cb11"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/62861 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc6563648dc146af9f7718ea7be9d331"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style> .dataframe th, .dataframe tbody td { text-align: left; padding-right: 30px; } </style> <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>answer</th>\n      <th>question</th>\n      <th>context</th>\n      <th>prompt</th>\n      <th>input_ids</th>\n      <th>attention_mask</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>SELECT perth FROM table_name_56 WHERE gold_coast = \"yes\" AND sydney = \"yes\" AND melbourne = \"yes\" AND adelaide = \"yes\"</td>\n      <td>Which Perth has Gold Coast yes, Sydney yes, Melbourne yes, and Adelaide yes?</td>\n      <td>CREATE TABLE table_name_56 (perth VARCHAR, adelaide VARCHAR, melbourne VARCHAR, gold_coast VARCHAR, sydney VARCHAR)</td>\n      <td>You are a powerful text-to-SQL model. Given the SQL tables and natural language question, your job is to write SQL query that answers the question.<br><br>### Table:<br>CREATE TABLE table_name_56 (perth VARCHAR, adelaide VARCHAR, melbourne VARCHAR, gold_coast VARCHAR, sydney VARCHAR)<br><br>### Question:<br>Which Perth has Gold Coast yes, Sydney yes, Melbourne yes, and Adelaide yes?<br><br>### Response:<br>SELECT perth FROM table_name_56 WHERE gold_coast = \"yes\" AND sydney = \"yes\" AND melbourne = \"yes\" AND adelaide = \"yes\"</td>\n      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...]</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...]</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(\n    # Load the model with 4-bit quantization\n    load_in_4bit=True,\n    # Use double quantization\n    bnb_4bit_use_double_quant=True,\n    # Use 4-bit Normal Float for storing the base model weights in GPU memory\n    bnb_4bit_quant_type=\"nf4\",\n    # De-quantize the weights to 16-bit (Brain) float before the forward/backward pass\n    bnb_4bit_compute_dtype=torch.bfloat16,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=quantization_config)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T04:15:45.810477Z","iopub.execute_input":"2025-02-12T04:15:45.810723Z","iopub.status.idle":"2025-02-12T04:17:59.289059Z","shell.execute_reply.started":"2025-02-12T04:15:45.810678Z","shell.execute_reply":"2025-02-12T04:17:59.288167Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d5e1058cb214e519b4723a9475aede9"}},"metadata":{}},{"name":"stderr","text":"`low_cpu_mem_usage` was None, now default to True since model is quantized.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"480b1c6fad744c2b8ebf7b5cc22ebdee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47410de126684ca58d035a6c44850170"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed5dd49accf649079a21cfd5ce9a6bc4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56d9a4e98bc44dd98a79cb09ca86ca72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e595943bbdbb46a2bc9c4a9963ba9b71"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e52399151d848988c7ca9e3f6a8c048"}},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"import transformers\n\ntokenizer = AutoTokenizer.from_pretrained(base_model_id)\npipeline = transformers.pipeline(model=model, tokenizer=tokenizer, task=\"text-generation\")\n\nsample = test_dataset[1]\nprompt = PROMPT_TEMPLATE.format(\n    context=sample[\"context\"], question=sample[\"question\"], output=\"\"\n)  # Leave the answer part blank\n\nwith torch.no_grad():\n    response = pipeline(prompt, max_new_tokens=256, repetition_penalty=1.15, return_full_text=False)\n\ndisplay_table({\"prompt\": prompt, \"generated_query\": response[0][\"generated_text\"]})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T04:18:31.804207Z","iopub.execute_input":"2025-02-12T04:18:31.804959Z","iopub.status.idle":"2025-02-12T04:18:38.301164Z","shell.execute_reply.started":"2025-02-12T04:18:31.804918Z","shell.execute_reply":"2025-02-12T04:18:38.300445Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style> .dataframe th, .dataframe tbody td { text-align: left; padding-right: 30px; } </style> <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prompt</th>\n      <th>generated_query</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>You are a powerful text-to-SQL model. Given the SQL tables and natural language question, your job is to write SQL query that answers the question.<br><br>### Table:<br>CREATE TABLE table_name_61 (game INTEGER, opponent VARCHAR, record VARCHAR)<br><br>### Question:<br>What is the lowest numbered game against Phoenix with a record of 29-17?<br><br>### Response:<br></td>\n      <td>SELECT * FROM table_name_61 WHERE game = 3 AND opponent = 'Phoenix' AND record = '29-17';<br><br>### Explanation:<br>The answer is 3 because it is the lowest numbered game against Phoenix with a record of 29-17. The other games in this series were 4, 5, and 6.</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\n# Enabling gradient checkpointing, to make the training further efficient\nmodel.gradient_checkpointing_enable()\n# Set up the model for quantization-aware training e.g. casting layers, parameter freezing, etc.\nmodel = prepare_model_for_kbit_training(model)\n\npeft_config = LoraConfig(\n    task_type=\"CAUSAL_LM\",\n    # This is the rank of the decomposed matrices A and B to be learned during fine-tuning. A smaller number will save more GPU memory but might result in worse performance.\n    r=32,\n    # This is the coefficient for the learned ΔW factor, so the larger number will typically result in a larger behavior change after fine-tuning.\n    lora_alpha=64,\n    # Drop out ratio for the layers in LoRA adaptors A and B.\n    lora_dropout=0.1,\n    # We fine-tune all linear layers in the model. It might sound a bit large, but the trainable adapter size is still only **1.16%** of the whole model.\n    target_modules=[\n        \"q_proj\",\n        \"k_proj\",\n        \"v_proj\",\n        \"o_proj\",\n        \"gate_proj\",\n        \"up_proj\",\n        \"down_proj\",\n        \"lm_head\",\n    ],\n    # Bias parameters to train. 'none' is recommended to keep the original model performing equally when turning off the adapter.\n    bias=\"none\",\n)\n\npeft_model = get_peft_model(model, peft_config)\npeft_model.print_trainable_parameters()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T04:18:38.302264Z","iopub.execute_input":"2025-02-12T04:18:38.302539Z","iopub.status.idle":"2025-02-12T04:18:39.757704Z","shell.execute_reply.started":"2025-02-12T04:18:38.302518Z","shell.execute_reply":"2025-02-12T04:18:39.756934Z"}},"outputs":[{"name":"stdout","text":"trainable params: 85,041,152 || all params: 7,326,773,248 || trainable%: 1.1607\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"from datetime import datetime\n\nimport transformers\nfrom transformers import TrainingArguments\n\nimport mlflow\n\n# Comment-out this line if you are running the tutorial on Databricks\nmlflow.set_experiment(\"MLflow PEFT Tutorial\")\n\ntraining_args = TrainingArguments(\n    # Set this to mlflow for logging your training\n    report_to=\"mlflow\",\n    # Name the MLflow run\n    run_name=f\"Mistral-7B-SQL-QLoRA-{datetime.now().strftime('%Y-%m-%d-%H-%M-%s')}\",\n    # Replace with your output destination\n    output_dir=\"YOUR_OUTPUT_DIR\",\n    # For the following arguments, refer to https://huggingface.co/docs/transformers/main_classes/trainer\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    gradient_checkpointing=True,\n    optim=\"paged_adamw_8bit\",\n    bf16=True,\n    learning_rate=2e-5,\n    lr_scheduler_type=\"constant\",\n    max_steps=500,\n    save_steps=100,\n    logging_steps=100,\n    warmup_steps=5,\n    # https://discuss.huggingface.co/t/training-llama-with-lora-on-multiple-gpus-may-exist-bug/47005/3\n    ddp_find_unused_parameters=False,\n)\n\ntrainer = transformers.Trainer(\n    model=peft_model,\n    train_dataset=tokenized_train_dataset,\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n    args=training_args,\n)\n\n# use_cache=True is incompatible with gradient checkpointing.\npeft_model.config.use_cache = False\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T04:18:43.336084Z","iopub.execute_input":"2025-02-12T04:18:43.336398Z","iopub.status.idle":"2025-02-12T04:18:46.343417Z","shell.execute_reply.started":"2025-02-12T04:18:43.336372Z","shell.execute_reply":"2025-02-12T04:18:46.342766Z"}},"outputs":[{"name":"stderr","text":"2025/02/12 04:18:45 INFO mlflow.tracking.fluent: Experiment with name 'MLflow PEFT Tutorial' does not exist. Creating a new experiment.\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"tokenizer.pad_token = tokenizer.eos_token\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T04:21:05.192724Z","iopub.execute_input":"2025-02-12T04:21:05.193177Z","iopub.status.idle":"2025-02-12T07:38:28.812205Z","shell.execute_reply.started":"2025-02-12T04:21:05.193140Z","shell.execute_reply":"2025-02-12T07:38:28.811269Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 3:16:59, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.668700</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.518500</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.510000</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.498900</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.490900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n","output_type":"stream"},{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=500, training_loss=0.5374084625244141, metrics={'train_runtime': 11842.1755, 'train_samples_per_second': 0.338, 'train_steps_per_second': 0.042, 'total_flos': 4.4210388467712e+16, 'train_loss': 0.5374084625244141, 'epoch': 0.06363144666093984})"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"# Basically the same format as we applied to the dataset. However, the template only accepts {prompt} variable so both table and question need to be fed in there.\nprompt_template = \"\"\"You are a powerful text-to-SQL model. Given the SQL tables and natural language question, your job is to write SQL query that answers the question.\n\n{prompt}\n\n### Response:\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T07:39:55.910556Z","iopub.execute_input":"2025-02-12T07:39:55.910964Z","iopub.status.idle":"2025-02-12T07:39:55.914581Z","shell.execute_reply.started":"2025-02-12T07:39:55.910933Z","shell.execute_reply":"2025-02-12T07:39:55.913623Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"from mlflow.models import infer_signature\n\nsample = train_dataset[1]\n\n# MLflow infers schema from the provided sample input/output/params\nsignature = infer_signature(\n    model_input=sample[\"prompt\"],\n    model_output=sample[\"answer\"],\n    # Parameters are saved with default values if specified\n    params={\"max_new_tokens\": 256, \"repetition_penalty\": 1.15, \"return_full_text\": False},\n)\nsignature\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T07:39:57.459974Z","iopub.execute_input":"2025-02-12T07:39:57.460274Z","iopub.status.idle":"2025-02-12T07:39:57.698118Z","shell.execute_reply.started":"2025-02-12T07:39:57.460250Z","shell.execute_reply":"2025-02-12T07:39:57.697392Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"inputs: \n  [string (required)]\noutputs: \n  [string (required)]\nparams: \n  ['max_new_tokens': long (default: 256), 'repetition_penalty': double (default: 1.15), 'return_full_text': boolean (default: False)]"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"import mlflow\n\n# Get the ID of the MLflow Run that was automatically created above\nlast_run_id = mlflow.last_active_run().info.run_id\n\n# Save a tokenizer without padding because it is only needed for training\ntokenizer_no_pad = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True)\n\n# If you interrupt the training, uncomment the following line to stop the MLflow run\n# mlflow.end_run()\n\nwith mlflow.start_run(run_id=last_run_id):\n    mlflow.log_params(peft_config.to_dict())\n    mlflow.transformers.log_model(\n        transformers_model={\"model\": trainer.model, \"tokenizer\": tokenizer_no_pad},\n        prompt_template=prompt_template,\n        signature=signature,\n        artifact_path=\"model\",  # This is a relative path to save model files within MLflow run\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T07:39:59.954427Z","iopub.execute_input":"2025-02-12T07:39:59.954786Z","iopub.status.idle":"2025-02-12T07:40:14.884614Z","shell.execute_reply.started":"2025-02-12T07:39:59.954755Z","shell.execute_reply":"2025-02-12T07:40:14.883656Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n2025/02/12 07:40:00 INFO mlflow.transformers: Overriding save_pretrained to False for PEFT models, following the Transformers behavior. The PEFT adaptor and config will be saved, but the base model weights will not and reference to the HuggingFace Hub repository will be logged instead.\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n2025/02/12 07:40:03 INFO mlflow.transformers: Skipping saving pretrained model weights to disk as the save_pretrained argumentis set to False. The reference to the HuggingFace Hub repository mistralai/Mistral-7B-v0.1 will be logged instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/1.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f3e221d50ee41cfa3adebd53bb9939c"}},"metadata":{}},{"name":"stderr","text":"2025/02/12 07:40:07 INFO mlflow.transformers: text-generation pipelines saved with prompt templates have the `return_full_text` pipeline kwarg set to False by default. To override this behavior, provide a `model_config` dict with `return_full_text` set to `True` when saving the model.\n2025/02/12 07:40:07 WARNING mlflow.utils.requirements_utils: Found torch version (2.5.1+cu121) contains a local version label (+cu121). MLflow logged a pip requirement for this package as 'torch==2.5.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n2025/02/12 07:40:07 WARNING mlflow.utils.requirements_utils: Found torchvision version (0.20.1+cu121) contains a local version label (+cu121). MLflow logged a pip requirement for this package as 'torchvision==0.20.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n2025/02/12 07:40:07 INFO mlflow.transformers: A local checkpoint path or PEFT model is given as the `transformers_model`. To avoid loading the full model into memory, we don't infer the pip requirement for the model. Instead, we will use the default requirements, but it may not capture all required pip libraries for the model. Consider providing the pip requirements explicitly.\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"# You can find the ID of run in the Run detail page on MLflow UI\n# mlflow_model = mlflow.pyfunc.load_model(\"runs:/YOUR_RUN_ID/model\")\n# runs:/8d25fa22115a49d8a3f19f71ac656c09/artifacts/model\nmlflow_model = mlflow.pyfunc.load_model(\"mlruns/730291955186101037/8d25fa22115a49d8a3f19f71ac656c09/artifacts/model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T07:48:38.305650Z","iopub.execute_input":"2025-02-12T07:48:38.306101Z"}},"outputs":[{"name":"stderr","text":"2025/02/12 07:48:38 WARNING mlflow.transformers.model_io: Could not specify device parameter for this pipeline type.Falling back to loading the model with the default device.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"# We only input table and question, since system prompt is adeed in the prompt template.\ntest_prompt = \"\"\"\n### Table:\nCREATE TABLE table_name_50 (venue VARCHAR, away_team VARCHAR)\n\n### Question:\nWhen Essendon played away; where did they play?\n\"\"\"\n\n# Inference parameters like max_tokens_length are set to default values specified in the Model Signature\ngenerated_query = mlflow_model.predict(test_prompt)[0]\ndisplay_table({\"prompt\": test_prompt, \"generated_query\": generated_query})\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}