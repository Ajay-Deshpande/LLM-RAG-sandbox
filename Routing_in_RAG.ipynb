{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5kf7pATZ2_Fz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kf7pATZ2_Fz",
        "outputId": "1ad0d9fc-5e39-4090-b3f9-239109b71d2d"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/Ajay-Deshpande/pytube"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bc509da-52b2-49fc-bc45-e5fd75ff5fed",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bc509da-52b2-49fc-bc45-e5fd75ff5fed",
        "outputId": "4c135a1a-98a9-46e2-a945-b4a1895ca59c"
      },
      "outputs": [],
      "source": [
        "! pip install langchain_community tiktoken langchain-google-genai langchainhub langchain-huggingface chromadb langchain youtube-transcript-api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bc82800-5498-40be-86db-e1f6df8c86da",
      "metadata": {
        "id": "3bc82800-5498-40be-86db-e1f6df8c86da"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "os.environ['LANGSMITH_PROJECT'] = \"langsmith_project_ROUTING\"\n",
        "os.environ['LANGCHAIN_API_KEY'] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78e8ecbd-7ba1-4a2d-8ad5-4ce59e164d23",
      "metadata": {
        "id": "78e8ecbd-7ba1-4a2d-8ad5-4ce59e164d23"
      },
      "outputs": [],
      "source": [
        "os.environ[\"GOOGLE_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "04c2cf60-d636-4992-a021-1236f7688999",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04c2cf60-d636-4992-a021-1236f7688999",
        "outputId": "c5def85f-daa2-40f3-cfab-dc275c3a124b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py:3553: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
            "\n",
            "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
            "with: `from pydantic import BaseModel`\n",
            "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
            "\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ],
      "source": [
        "from typing import Literal\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.utils.math import cosine_similarity\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "\n",
        "# Data model\n",
        "class RouteQuery(BaseModel):\n",
        "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
        "\n",
        "    datasource: Literal[\"python_docs\", \"js_docs\", \"golang_docs\"] = Field(\n",
        "        ...,\n",
        "        description=\"Given a user question choose which datasource would be most relevant for answering their question\",\n",
        "    )\n",
        "\n",
        "# LLM with function call\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0)\n",
        "structured_llm = llm.with_structured_output(RouteQuery)\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"You are an expert at routing a user question to the appropriate data source.\n",
        "\n",
        "Based on the programming language the question is referring to, route it to the relevant data source.\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define router\n",
        "router = prompt | structured_llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "cfc6febc-93df-49b4-9920-c93589ba021e",
      "metadata": {
        "id": "cfc6febc-93df-49b4-9920-c93589ba021e"
      },
      "outputs": [],
      "source": [
        "question = \"\"\"Why doesn't the following code work:\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\"human\", \"speak in {language}\"])\n",
        "prompt.invoke(\"french\")\n",
        "\"\"\"\n",
        "\n",
        "result = router.invoke({\"question\": question})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "277536df-0904-4d99-92bb-652621afbdec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "277536df-0904-4d99-92bb-652621afbdec",
        "outputId": "371d54a9-f7ec-4e73-c1d3-12efe4350cfc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RouteQuery(datasource='python_docs')"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "636a43ae-50f3-43a1-a1b7-93266ea13bcd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "636a43ae-50f3-43a1-a1b7-93266ea13bcd",
        "outputId": "8d34e247-a384-4ee0-840b-7ac9cc0e984b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'python_docs'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result.datasource"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "01f15722-35c6-4456-ad1b-06463233db25",
      "metadata": {
        "id": "01f15722-35c6-4456-ad1b-06463233db25"
      },
      "outputs": [],
      "source": [
        "def choose_route(result):\n",
        "    if \"python_docs\" in result.datasource.lower():\n",
        "        ### Logic here\n",
        "        return \"chain for python_docs\"\n",
        "    elif \"js_docs\" in result.datasource.lower():\n",
        "        ### Logic here\n",
        "        return \"chain for js_docs\"\n",
        "    else:\n",
        "        ### Logic here\n",
        "        return \"golang_docs\"\n",
        "\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "full_chain = router | RunnableLambda(choose_route)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "6af07b77-0537-4635-87ec-ad8f59d34e9b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6af07b77-0537-4635-87ec-ad8f59d34e9b",
        "outputId": "c99adf99-db44-456a-9463-28464e29c936"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'chain for python_docs'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "full_chain.invoke({\"question\": question})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb1db843-95f4-4abd-8d4c-4a41f0910949",
      "metadata": {
        "id": "cb1db843-95f4-4abd-8d4c-4a41f0910949"
      },
      "source": [
        "### Semantic routing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "53cbfa72-c35a-4d1d-aa6d-08a570ab2170",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53cbfa72-c35a-4d1d-aa6d-08a570ab2170",
        "outputId": "b96e4aec-aba1-4365-8bda-32b989461c00"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using PHYSICS\n",
            "A black hole is a region of spacetime where gravity is so strong that nothing, not even light, can escape. The boundary of this region is called the event horizon. Inside the event horizon, spacetime is so curved that all paths lead to the singularity at the center of the black hole. The singularity is a point of infinite density and gravity.\n",
            "\n",
            "Black holes are formed when massive stars collapse at the end of their lives. As the star collapses, its gravity becomes so strong that it pulls everything in, including light. The star eventually collapses into a single point, called a singularity. The singularity is surrounded by an event horizon.\n",
            "\n",
            "Black holes are invisible because they do not emit any light. However, they can be detected by their gravitational effects on nearby objects. For example, black holes can cause stars to orbit them in a peculiar way. They can also cause gas and dust to form a disk around them, which emits X-rays.\n",
            "\n",
            "Black holes are one of the most mysterious objects in the universe. They are still not fully understood, but scientists are learning more about them all the time.\n"
          ]
        }
      ],
      "source": [
        "# Two prompts\n",
        "physics_template = \"\"\"You are a very smart physics professor. \\\n",
        "You are great at answering questions about physics in a concise and easy to understand manner. \\\n",
        "When you don't know the answer to a question you admit that you don't know.\n",
        "\n",
        "Here is a question:\n",
        "{query}\"\"\"\n",
        "\n",
        "math_template = \"\"\"You are a very good mathematician. You are great at answering math questions. \\\n",
        "You are so good because you are able to break down hard problems into their component parts, \\\n",
        "answer the component parts, and then put them together to answer the broader question.\n",
        "\n",
        "Here is a question:\n",
        "{query}\"\"\"\n",
        "\n",
        "# Embed prompts\n",
        "embeddings = HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-mpnet-base-v2\")\n",
        "prompt_templates = [physics_template, math_template]\n",
        "prompt_embeddings = embeddings.embed_documents(prompt_templates)\n",
        "\n",
        "# Route question to prompt\n",
        "def prompt_router(input):\n",
        "    # Embed question\n",
        "    query_embedding = embeddings.embed_query(input[\"query\"])\n",
        "    # Compute similarity\n",
        "    similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]\n",
        "    most_similar = prompt_templates[similarity.argmax()]\n",
        "    # Chosen prompt\n",
        "    print(\"Using MATH\" if most_similar == math_template else \"Using PHYSICS\")\n",
        "    return PromptTemplate.from_template(most_similar)\n",
        "\n",
        "\n",
        "chain = (\n",
        "    {\"query\": RunnablePassthrough()}\n",
        "    | RunnableLambda(prompt_router)\n",
        "    | ChatGoogleGenerativeAI(model = \"gemini-1.0-pro\")\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(chain.invoke(\"What's a black hole\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e93d3516-3f77-4548-b55f-5db4d7c9fbbb",
      "metadata": {
        "id": "e93d3516-3f77-4548-b55f-5db4d7c9fbbb"
      },
      "source": [
        "## Query structuring for metadata filters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "b22eb666-a6c2-4b3f-81dd-93ece81f035d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b22eb666-a6c2-4b3f-81dd-93ece81f035d",
        "outputId": "ee943c99-9cf1-46e0-8d9e-8bac405a56ec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'source': 'pbAd8O1Lvm4',\n",
              " 'title': 'Title Unavailable',\n",
              " 'description': 'Unknown',\n",
              " 'view_count': 0,\n",
              " 'thumbnail_url': 'https://img.youtube.com/vi/pbAd8O1Lvm4/maxresdefault.jpg',\n",
              " 'publish_date': '2024-02-07 00:00:00',\n",
              " 'length': 0,\n",
              " 'author': 'unknown'}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_community.document_loaders import YoutubeLoader\n",
        "\n",
        "docs = YoutubeLoader.from_youtube_url(\n",
        "    \"https://www.youtube.com/watch?v=pbAd8O1Lvm4\", add_video_info=True\n",
        ").load()\n",
        "\n",
        "docs[0].metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "7731745b-accc-4cf1-8291-e12d1aa46361",
      "metadata": {
        "id": "7731745b-accc-4cf1-8291-e12d1aa46361"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "from typing import Literal, Optional, Tuple\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "class TutorialSearch(BaseModel):\n",
        "    \"\"\"Search over a database of tutorial videos about a software library.\"\"\"\n",
        "\n",
        "    content_search: str = Field(\n",
        "        ...,\n",
        "        description=\"Similarity search query applied to video transcripts.\",\n",
        "    )\n",
        "    title_search: str = Field(\n",
        "        ...,\n",
        "        description=(\n",
        "            \"Alternate version of the content search query to apply to video titles. \"\n",
        "            \"Should be succinct and only include key words that could be in a video \"\n",
        "            \"title.\"\n",
        "        ),\n",
        "    )\n",
        "    min_view_count: Optional[int] = Field(\n",
        "        None,\n",
        "        description=\"Minimum view count filter, inclusive. Only use if explicitly specified.\",\n",
        "    )\n",
        "    max_view_count: Optional[int] = Field(\n",
        "        None,\n",
        "        description=\"Maximum view count filter, exclusive. Only use if explicitly specified.\",\n",
        "    )\n",
        "    earliest_publish_date: Optional[datetime.date] = Field(\n",
        "        None,\n",
        "        description=\"Earliest publish date filter, inclusive. Only use if explicitly specified.\",\n",
        "    )\n",
        "    latest_publish_date: Optional[datetime.date] = Field(\n",
        "        None,\n",
        "        description=\"Latest publish date filter, exclusive. Only use if explicitly specified.\",\n",
        "    )\n",
        "    min_length_sec: Optional[int] = Field(\n",
        "        None,\n",
        "        description=\"Minimum video length in seconds, inclusive. Only use if explicitly specified.\",\n",
        "    )\n",
        "    max_length_sec: Optional[int] = Field(\n",
        "        None,\n",
        "        description=\"Maximum video length in seconds, exclusive. Only use if explicitly specified.\",\n",
        "    )\n",
        "\n",
        "    def pretty_print(self) -> None:\n",
        "        for field in self.__fields__:\n",
        "            if getattr(self, field) is not None and getattr(self, field) != getattr(\n",
        "                self.__fields__[field], \"default\", None\n",
        "            ):\n",
        "                print(f\"{field}: {getattr(self, field)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "f699d9e7-468e-4574-bdba-f4be4a5779de",
      "metadata": {
        "id": "f699d9e7-468e-4574-bdba-f4be4a5779de"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "system = \"\"\"You are an expert at converting user questions into database queries. \\\n",
        "You have access to a database of tutorial videos about a software library for building LLM-powered applications. \\\n",
        "Given a question, return a database query optimized to retrieve the most relevant results.\n",
        "\n",
        "If there are acronyms or words you are not familiar with, do not try to rephrase them.\"\"\"\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0)\n",
        "structured_llm = llm.with_structured_output(TutorialSearch)\n",
        "query_analyzer = prompt | structured_llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "1b776858-a589-4fe5-a8a3-19530706075d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b776858-a589-4fe5-a8a3-19530706075d",
        "outputId": "a8886ad0-b81f-4897-9cd1-917444933bde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content_search: rag from scratch\n",
            "title_search: rag from scratch\n"
          ]
        }
      ],
      "source": [
        "query_analyzer.invoke({\"question\": \"rag from scratch\"}).pretty_print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "65bfad12-1985-433e-a980-eb8c9da53f72",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65bfad12-1985-433e-a980-eb8c9da53f72",
        "outputId": "e31f67e8-8ef3-4438-e70b-92520f77be55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content_search: chat langchain\n",
            "title_search: chat langchain\n",
            "earliest_publish_date: 2023-01-01\n",
            "latest_publish_date: 2024-01-01\n"
          ]
        }
      ],
      "source": [
        "query_analyzer.invoke(\n",
        "    {\"question\": \"videos on chat langchain published in 2023\"}\n",
        ").pretty_print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "99643372-01cc-49cc-a507-bebbed096247",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99643372-01cc-49cc-a507-bebbed096247",
        "outputId": "9cea4b92-114c-49e4-9e92-6cdf128b04e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content_search: chat langchain\n",
            "title_search: chat langchain\n",
            "latest_publish_date: 1970-01-01\n"
          ]
        }
      ],
      "source": [
        "query_analyzer.invoke(\n",
        "    {\"question\": \"videos that are focused on the topic of chat langchain that are published before 2024\"}\n",
        ").pretty_print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "c26f2329-d091-4a47-995a-822e3f062ea1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c26f2329-d091-4a47-995a-822e3f062ea1",
        "outputId": "f9cfa044-aaf9-44db-9b4e-90142353522c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content_search: how to use multi-modal models in an agent\n",
            "title_search: multi-modal agent\n",
            "max_length_sec: 300\n"
          ]
        }
      ],
      "source": [
        "query_analyzer.invoke(\n",
        "    {\n",
        "        \"question\": \"how to use multi-modal models in an agent, only videos under 5 minutes\"\n",
        "    }\n",
        ").pretty_print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d62d7164-7949-4369-80ae-b6bd1b2a08ea",
      "metadata": {
        "id": "d62d7164-7949-4369-80ae-b6bd1b2a08ea"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
